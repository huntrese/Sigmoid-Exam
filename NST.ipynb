{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision Pillow -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDa-EIGRugDg",
        "outputId": "104c2f00-ef66-4c1a-9f1e-0b4666465814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (17.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Oy71tWZCwpJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.rand(2,3).cuda())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7SaZ_pAwIUg",
        "outputId": "63a6e8a2-fa2d-489a-826b-0f7d6bc720de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9027, 0.1280, 0.0288],\n",
            "        [0.7828, 0.9218, 0.0314]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def load_image(image_path, transform=None, max_size=None):\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    if max_size:\n",
        "        width, height = image.size\n",
        "        if width > height:\n",
        "            new_width = max_size\n",
        "            new_height = int(height * (max_size / width))\n",
        "        else:\n",
        "            new_width = int(width * (max_size / height))\n",
        "            new_height = max_size\n",
        "\n",
        "        image = image.resize((new_width, new_height), Image.LANCZOS)\n",
        "\n",
        "    if transform:\n",
        "        image = transform(image).unsqueeze(0)\n",
        "\n",
        "    return image.to(device)\n",
        "\n",
        "class VGGNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGGNet, self).__init__()\n",
        "        self.select = ['0', '5', '10', '19', '28']\n",
        "        self.vgg = models.vgg19(pretrained=True).features\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        for name, layer in self.vgg._modules.items():\n",
        "            x = layer(x)\n",
        "            if name in self.select:\n",
        "                features.append(x)\n",
        "        return features\n",
        "\n",
        "\n",
        "def main(content_path, style_path, max_size=400, total_step=4000, log_step=80, sample_step=800, style_weight=100, lr=0.001):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.472, 0.436, 0.424), std=(0.227, 0.226, 0.225))\n",
        "    ])\n",
        "\n",
        "    content = load_image(content_path, transform, max_size=256)\n",
        "    style = load_image(style_path, transform, max_size=256)  # Resize style image to match content dimensions\n",
        "\n",
        "    target = content.clone().requires_grad_(True)\n",
        "\n",
        "    optimizer = torch.optim.Adam([target], lr=lr, betas=[0.5, 0.999])\n",
        "    vgg = VGGNet().to(device).eval()\n",
        "\n",
        "    for step in range(total_step):\n",
        "        target_features = vgg(target)\n",
        "        content_features = vgg(content)\n",
        "        style_features = vgg(style)\n",
        "\n",
        "        style_loss = 0\n",
        "        content_loss = 0\n",
        "        for f1, f2, f3 in zip(target_features, content_features, style_features):\n",
        "            content_loss += torch.mean((f1 - f2) ** 2)\n",
        "\n",
        "            # Check dimensions of f1 and f3\n",
        "            _, c, h, w = f1.size()\n",
        "            f1 = f1.view(c, -1)  # Reshape f1 to (c, h*w)\n",
        "            f3 = f3.view(c, -1)  # Reshape f3 to (c, h*w)\n",
        "\n",
        "            # Calculate Gram matrix for f1 and f3\n",
        "            f1 = torch.mm(f1, f1.t())\n",
        "            f3 = torch.mm(f3, f3.t())\n",
        "\n",
        "            style_loss += torch.mean((f1 - f3) ** 2) / (c * h * w)\n",
        "\n",
        "\n",
        "\n",
        "        loss = content_loss + style_weight * style_loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (step+1) % log_step == 0:\n",
        "            print('Step [{}/{}], Content Loss: {:.4f}, Style Loss: {:.4f}'.format(step+1, total_step, content_loss.item(), style_loss.item()))\n",
        "\n",
        "        if (step+1) % sample_step == 0:\n",
        "            denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n",
        "            img = target.clone().squeeze()\n",
        "            img = denorm(img).clamp_(0, 1)\n",
        "            img = img.cpu().detach().numpy()\n",
        "            img = np.transpose(img, (1, 2, 0))\n",
        "            img = (img * 255).astype(np.uint8)\n",
        "            img = Image.fromarray(img)\n",
        "            img.save('output-{}.png'.format(step+1))\n",
        "\n",
        "# You can specify the paths to your content and style images here\n",
        "content_path = '/content/12472_Comp_E1_ImageA.jpg'\n",
        "style_path = '/content/download-_1_.png'\n",
        "\n",
        "main(content_path, style_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZHwc3MAjuW2",
        "outputId": "4eabc156-6f61-407c-8ec6-d033f6de6203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step [80/4000], Content Loss: 11.0362, Style Loss: 206.3892\n",
            "Step [160/4000], Content Loss: 16.5164, Style Loss: 133.2332\n",
            "Step [240/4000], Content Loss: 19.9567, Style Loss: 97.7935\n",
            "Step [320/4000], Content Loss: 22.5299, Style Loss: 76.2136\n",
            "Step [400/4000], Content Loss: 24.5207, Style Loss: 61.5953\n",
            "Step [480/4000], Content Loss: 26.1519, Style Loss: 51.0329\n",
            "Step [560/4000], Content Loss: 27.5391, Style Loss: 43.0825\n",
            "Step [640/4000], Content Loss: 28.7737, Style Loss: 36.9377\n",
            "Step [720/4000], Content Loss: 29.8577, Style Loss: 32.0678\n",
            "Step [800/4000], Content Loss: 30.8445, Style Loss: 28.1310\n",
            "Step [880/4000], Content Loss: 31.7401, Style Loss: 24.8980\n",
            "Step [960/4000], Content Loss: 32.5444, Style Loss: 22.2054\n",
            "Step [1040/4000], Content Loss: 33.2919, Style Loss: 19.9351\n",
            "Step [1120/4000], Content Loss: 33.9825, Style Loss: 18.0039\n",
            "Step [1200/4000], Content Loss: 34.6085, Style Loss: 16.3488\n",
            "Step [1280/4000], Content Loss: 35.1890, Style Loss: 14.9166\n",
            "Step [1360/4000], Content Loss: 35.7314, Style Loss: 13.6705\n",
            "Step [1440/4000], Content Loss: 36.2504, Style Loss: 12.5782\n",
            "Step [1520/4000], Content Loss: 36.7345, Style Loss: 11.6159\n",
            "Step [1600/4000], Content Loss: 37.1885, Style Loss: 10.7621\n",
            "Step [1680/4000], Content Loss: 37.6193, Style Loss: 10.0024\n",
            "Step [1760/4000], Content Loss: 38.0362, Style Loss: 9.3215\n",
            "Step [1840/4000], Content Loss: 38.4549, Style Loss: 8.7083\n",
            "Step [1920/4000], Content Loss: 38.8644, Style Loss: 8.1534\n",
            "Step [2000/4000], Content Loss: 39.2461, Style Loss: 7.6509\n",
            "Step [2080/4000], Content Loss: 39.6203, Style Loss: 7.1939\n",
            "Step [2160/4000], Content Loss: 39.9745, Style Loss: 6.7773\n",
            "Step [2240/4000], Content Loss: 40.3153, Style Loss: 6.3965\n",
            "Step [2320/4000], Content Loss: 40.6399, Style Loss: 6.0471\n",
            "Step [2400/4000], Content Loss: 40.9458, Style Loss: 5.7264\n",
            "Step [2480/4000], Content Loss: 41.2263, Style Loss: 5.4320\n",
            "Step [2560/4000], Content Loss: 41.4853, Style Loss: 5.1615\n",
            "Step [2640/4000], Content Loss: 41.7311, Style Loss: 4.9118\n",
            "Step [2720/4000], Content Loss: 41.9682, Style Loss: 4.6803\n",
            "Step [2800/4000], Content Loss: 42.1907, Style Loss: 4.4657\n",
            "Step [2880/4000], Content Loss: 42.4009, Style Loss: 4.2668\n",
            "Step [2960/4000], Content Loss: 42.6117, Style Loss: 4.0824\n",
            "Step [3040/4000], Content Loss: 42.8042, Style Loss: 3.9104\n",
            "Step [3120/4000], Content Loss: 42.9906, Style Loss: 3.7500\n",
            "Step [3200/4000], Content Loss: 43.1613, Style Loss: 3.6003\n",
            "Step [3280/4000], Content Loss: 43.3270, Style Loss: 3.4603\n",
            "Step [3360/4000], Content Loss: 43.4805, Style Loss: 3.3295\n",
            "Step [3440/4000], Content Loss: 43.6283, Style Loss: 3.2067\n",
            "Step [3520/4000], Content Loss: 43.7821, Style Loss: 3.0910\n",
            "Step [3600/4000], Content Loss: 43.9213, Style Loss: 2.9823\n",
            "Step [3680/4000], Content Loss: 44.0472, Style Loss: 2.8795\n",
            "Step [3760/4000], Content Loss: 44.1633, Style Loss: 2.7827\n",
            "Step [3840/4000], Content Loss: 44.2722, Style Loss: 2.6914\n",
            "Step [3920/4000], Content Loss: 44.3786, Style Loss: 2.6051\n",
            "Step [4000/4000], Content Loss: 44.4864, Style Loss: 2.5231\n"
          ]
        }
      ]
    }
  ]
}